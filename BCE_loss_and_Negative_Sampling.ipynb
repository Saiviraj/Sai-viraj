{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saiviraj/Sai-viraj/blob/main/BCE_loss_and_Negative_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hPaqnWDhqp4D"
      },
      "outputs": [],
      "source": [],
      "id": "hPaqnWDhqp4D"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEu0dojU-m_W",
        "outputId": "aa4c6c3f-81ea-4905-9e08-a08d62063556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ],
      "id": "MEu0dojU-m_W"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1f2f474-442e-4b5e-883e-6dd424b713a1",
        "outputId": "9afa03e2-4f60-404a-be63-046e4c40f375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n",
            "Drive already mounted at /content/ydrive/; to attempt to forcibly remount, call drive.mount(\"/content/ydrive/\", force_remount=True).\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch as th\n",
        "from torch.autograd import Variable as V\n",
        "from torch import nn,optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "%load_ext google.colab.data_table\n",
        "from google.colab import data_table\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/ydrive/\")\n",
        "# os.chdir(\"Your Location here\")\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "data_folder = '/content/drive/'"
      ],
      "id": "c1f2f474-442e-4b5e-883e-6dd424b713a1"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "b23b740a-feba-48a5-8e05-a72ece218e9d",
        "outputId": "1584b70f-5efb-4d3b-ffad-1cc1c7bdcd05"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-fc78433fff14>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Darknet.CSV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#current_directory = os.getcwd()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(current_directory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Darknet.CSV'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(data_folder + 'Darknet.CSV', on_bad_lines='skip')\n",
        "#current_directory = os.getcwd()\n",
        "#print(current_directory)\n",
        "print(df.shape)\n",
        "\n",
        "df.iloc[:5]"
      ],
      "id": "b23b740a-feba-48a5-8e05-a72ece218e9d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS6U7fjtQiVr"
      },
      "source": [],
      "id": "iS6U7fjtQiVr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aed9066-a376-40f9-98a2-7c57b1e48aa2"
      },
      "outputs": [],
      "source": [
        "print('Total Number of flows:',len(df['Flow ID'].unique()))\n",
        "print('Obs per flow, 10 largets:\\n', df['Flow ID'].value_counts().value_counts()[:10])\n",
        "\n",
        "df['src-dst'] = df['Src IP']+'-'+df['Dst IP']\n",
        "print('Number of unique flows per Flow ID:\\n', df.groupby(['Flow ID'])['src-dst'].nunique().value_counts())\n",
        "\n",
        "df['src-dst-dst port-protocol'] = df['Src IP']+'-'+df['Dst IP']+'-'+df['Dst Port'].astype(str)+'-'+df['Protocol'].astype(str)\n",
        "print('Number of unique flow charactaristics per Flow ID:\\n', df.groupby(['Flow ID'])['src-dst-dst port-protocol'].nunique().value_counts())\n",
        "\n",
        "filtered_df = df[['Src IP', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp']].drop_duplicates()\n",
        "filtered_df['Dst Port'] = filtered_df['Dst Port'].astype(str)\n",
        "filtered_df['Protocol'] = filtered_df['Protocol'].astype(str)\n",
        "filtered_df.iloc[:5]\n"
      ],
      "id": "0aed9066-a376-40f9-98a2-7c57b1e48aa2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0023f56-bb02-44f0-b8be-378e7e3e9c62"
      },
      "outputs": [],
      "source": [
        "def gen_sameples_and_vocab_from_a_flow(df, flatten=True, source_ip = 'Src IP', dest_ip = 'Dst IP', proto = 'Protocol', dest_port = 'Dst Port'):\n",
        "    \"\"\"\n",
        "    Create pairs from a Flow dataset and vocabulary\n",
        "    \"\"\"\n",
        "    df = df.reset_index(drop=True)\n",
        "    pairs = [(\n",
        "        (df[source_ip][j],df[dest_ip][j]),\n",
        "        (df[source_ip][j],df[dest_port][j]),\n",
        "        (df[source_ip][j],df[proto][j]),\n",
        "        (df[dest_port][j], df[dest_ip][j]),\n",
        "        (df[proto][j], df[dest_ip][j])\n",
        "     ) for j in range(len(df))]\n",
        "\n",
        "    if flatten:\n",
        "        pairs = [item for sublist in pairs for item in sublist]\n",
        "\n",
        "    vocab = set(df[source_ip].tolist() + df[dest_ip].tolist() + df[dest_port].tolist() + df[proto].tolist())\n",
        "    vocab_size = len(vocab)\n",
        "    word_to_ind = {word:idx for idx,word in enumerate(vocab)}\n",
        "    ind_to_word = {idx:word for idx,word in enumerate(vocab)}\n",
        "\n",
        "    return pairs, vocab, vocab_size, word_to_ind, ind_to_word\n",
        "\n",
        "pairs, vocab, vocab_size, word_to_ind, ind_to_word = gen_sameples_and_vocab_from_a_flow(filtered_df)\n",
        "\n",
        "print(f'Vocabulary size: {vocab_size}')\n",
        "print('First five elements:\\n', list(vocab)[:5])\n",
        "print('First five nested pairs:')\n",
        "pairs[:10]\n",
        "\n"
      ],
      "id": "a0023f56-bb02-44f0-b8be-378e7e3e9c62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVri579cSo88"
      },
      "outputs": [],
      "source": [
        "corpus = pairs\n",
        "voc_size = vocab_size\n",
        "\n",
        "print(voc_size)\n",
        "corpus[:5]"
      ],
      "id": "aVri579cSo88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqyRIP4ZjoV5"
      },
      "outputs": [],
      "source": [
        "t = 60\n",
        "power_parameter = 0.75\n",
        "ns_table_size = 100000\n",
        "\n",
        "def make_ns_table_ips(pairs, t=60, power_parameter=0.75, ns_table_size=100000):\n",
        "  \"\"\"\n",
        "  pairs - pairs of entities. Probably (ip_source,ip_destination) in our real data.\n",
        "  t - threshold variable from the paper.\n",
        "  \"\"\"\n",
        "  from collections import Counter\n",
        "  import operator\n",
        "\n",
        "  flat_list_of_pairs = [item for sublist in pairs for item in sublist]\n",
        "  counter = Counter(flat_list_of_pairs)\n",
        "\n",
        "  # replace all the counts for cases where #.times of appearence < t --> 0\n",
        "  increasingly_sorted_counter = dict(sorted(counter.items(), key=operator.itemgetter(1)))\n",
        "  increasingly_sorted_counter_keys = list(increasingly_sorted_counter.keys())\n",
        "  increasingly_sorted_counter_values = np.array(list(increasingly_sorted_counter.values()))\n",
        "  increasingly_sorted_counter_values[increasingly_sorted_counter_values<t] = 0\n",
        "  increasingly_sorted_counter_thresholded = {i : j for i, j in zip(increasingly_sorted_counter_keys, increasingly_sorted_counter_values)}\n",
        "  decreasingly_sorted_counter_thresholded = dict(sorted(increasingly_sorted_counter_thresholded.items(), key=operator.itemgetter(1),reverse=True))\n",
        "\n",
        "  # convert to frequencies rather than counts\n",
        "  # S = np.sum(np.array(list(decreasingly_sorted_counter_thresholded.values())))\n",
        "  S = 1\n",
        "  decreasingly_sorted_counter_thresholded_freq_values = [decreasingly_sorted_counter_thresholded[k]/S for k in decreasingly_sorted_counter_thresholded.keys()]\n",
        "  decreasingly_sorted_counter_thresholded_keys = list(decreasingly_sorted_counter_thresholded.keys())\n",
        "\n",
        "  # Create negative_sampling table\n",
        "  # The negative sampling probabilities are proportional to the frequencies\n",
        "  # to the power of a constant (typically 0.75).\n",
        "  freqs_sorted = [(decreasingly_sorted_counter_thresholded_keys[j], decreasingly_sorted_counter_thresholded_freq_values[j]) for j in range(len(decreasingly_sorted_counter_thresholded_keys))]\n",
        "  ns_table = {}\n",
        "  sum_freq = 0\n",
        "  for w, freq in freqs_sorted:\n",
        "      ns_freq = freq ** power_parameter\n",
        "      ns_table[w] = ns_freq\n",
        "      sum_freq += ns_freq\n",
        "\n",
        "  # Convert the negative sampling probabilities to integers, in order to make\n",
        "  # sampling a bit faster and easier.\n",
        "  # We return a list of tuples consisting of:\n",
        "  # - the word\n",
        "  # - its frequency in the training data\n",
        "  # - the number of positions reserved for this word in the negative sampling table\n",
        "  scaler = ns_table_size / sum_freq\n",
        "  results = [(w, freq, int(round(ns_table[w]*scaler))) for w, freq in freqs_sorted]\n",
        "  return results\n",
        "\n"
      ],
      "id": "fqyRIP4ZjoV5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUfDbzHpWtsX"
      },
      "outputs": [],
      "source": [
        "ns_table = make_ns_table_ips(pairs)\n",
        "\n",
        "print('Sum:', np.sum([x[2] for x in ns_table]))\n",
        "ns_table[:5]"
      ],
      "id": "IUfDbzHpWtsX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fY0OpPU5467"
      },
      "outputs": [],
      "source": [
        "k = 6\n",
        "\n",
        "unflattened_list = [[x[0]]*x[2] for x in ns_table]\n",
        "flattened_list = np.array([item for sublist in unflattened_list for item in sublist])\n",
        "false_entities = np.random.choice(flattened_list, size=k)\n"
      ],
      "id": "6fY0OpPU5467"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdQt-_BkLQqt"
      },
      "outputs": [],
      "source": [
        "pairs_batch = pairs[:16]\n",
        "pairs_batch"
      ],
      "id": "cdQt-_BkLQqt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWAfRGF7VzEg"
      },
      "outputs": [],
      "source": [
        "class ContextGenerator:\n",
        "  def __init__(self, ns_table, k=20):\n",
        "\n",
        "    # Cerate a repeated list of negative samples, using ns_table\n",
        "    unflattened_list = [[x[0]]*x[2] for x in ns_table]\n",
        "    flattened_list = np.array([item for sublist in unflattened_list for item in sublist])\n",
        "    self.ns_table = flattened_list\n",
        "    self.k = k\n",
        "\n",
        "  def create_batch(self, true_pairs):\n",
        "    \"\"\"\n",
        "    Create a batch of examples to be entered later to the ip2vec model.\n",
        "    - pairs_batch: a list of true pairs, containing pairs of entities like described in\n",
        "      the ip2vec paper [(ip1,ip2)(ip1,destination_protocol2),...].\n",
        "    - ns_table: A list of different entities from the data, with their empirical\n",
        "      probabilities, and with the number of times they are to repeat in the table\n",
        "      that we will train on.\n",
        "\n",
        "    stages:\n",
        "    1. Given desired number of examples:\n",
        "      - Sample alpha in [1, int(theta*<batch_size>)] to make the number of true\n",
        "        pairs in the batch. The rest of the sample [int(0.25*<batch_size>), (<batch_size>-1)]\n",
        "        will be the false pairs to be generated.\n",
        "      - Draw <alpha> number of obs from pairs\n",
        "      - Draw (<batch_size>-<alpha>) ip addresses from ns_table.\n",
        "      - Choose with replacement ip addresses from the true pairs and assign them with\n",
        "        new destinations.\n",
        "\n",
        "    \"\"\"\n",
        "    # unpack from self\n",
        "    ns_table = self.ns_table\n",
        "    k = self.k\n",
        "    batch_size = len(true_pairs)\n",
        "\n",
        "    # define resulting_batch\n",
        "    resulting_batch = []\n",
        "\n",
        "    # add positive pairs to batch\n",
        "    for pair in true_pairs:\n",
        "      resulting_batch.append([pair[0],pair[1], 1])\n",
        "\n",
        "    # keep only sources\n",
        "    sources = [x[0] for x in true_pairs]\n",
        "\n",
        "    # Draw batch_size-alpha false entities\n",
        "    false_entities = np.random.choice(flattened_list, size=(k*batch_size))\n",
        "\n",
        "    # add negative pairs to batch\n",
        "    for j in range(len(false_entities)):\n",
        "      resulting_batch.append([np.random.choice(sources), false_entities[j], 0])\n",
        "\n",
        "    return np.array(resulting_batch)\n",
        "\n"
      ],
      "id": "WWAfRGF7VzEg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x5TKUWoeNct"
      },
      "outputs": [],
      "source": [
        "context_generator = ContextGenerator(ns_table=ns_table)\n",
        "context_generator.create_batch(true_pairs=pairs[:5])\n"
      ],
      "id": "5x5TKUWoeNct"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAOKnlqTlRdY"
      },
      "outputs": [],
      "source": [
        "batch = context_generator.create_batch(true_pairs=pairs[:5])\n",
        "batch[:,2]"
      ],
      "id": "VAOKnlqTlRdY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTlSxQhn9Ndb"
      },
      "outputs": [],
      "source": [
        "batch[:,2].astype(int)"
      ],
      "id": "OTlSxQhn9Ndb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvsmucKgxdXl"
      },
      "outputs": [],
      "source": [
        "class IP2Vec(nn.Module):\n",
        "  def __init__(self, vocab_size, word_to_ind, embedding_dim=128):\n",
        "    super().__init__()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.device = device\n",
        "    self.word_to_ind = word_to_ind\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.u_embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "    self.v_embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "    self.log_sigmoid = nn.LogSigmoid()\n",
        "\n",
        "    init_range= 0.5/embedding_dim\n",
        "    self.u_embedding.weight.data.uniform_(-init_range,init_range)\n",
        "    self.v_embedding.weight.data.uniform_(-0,0)\n",
        "\n",
        "    # Target word embeddings\n",
        "    self.w = nn.Embedding(voc_size, embedding_dim)\n",
        "    # Context embeddings\n",
        "    self.c = nn.Embedding(voc_size, embedding_dim)\n",
        "\n",
        "  def get_ip_embeddings(self,ip):\n",
        "    ip = torch.tensor([word_to_ind[ip]])\n",
        "    ip = ip.to(self.device)\n",
        "    return self.u_embedding(ip).view(1,-1)\n",
        "\n",
        "  def forward(self, targets, contexts): # target, context,neg\n",
        "    # batch looks like the following:\n",
        "    #   array([[a,b,1],\n",
        "    #          [c,d,1],\n",
        "    #          [c,m,0],\n",
        "    #          [c,g,0]])\n",
        "\n",
        "\n",
        "    # Look up the embeddings for the target words.\n",
        "    # shape: (batch size, embedding dimension)\n",
        "    tagets_embeddings = self.u_embedding(targets)\n",
        "    n_batch, _ = tagets_embeddings.shape\n",
        "\n",
        "    # View this as a 3-dimensional tensor, with\n",
        "    # shape (batch size, 1, embedding dimension)\n",
        "    tagets_embeddings = tagets_embeddings.view(n_batch, 1, self.embedding_dim)\n",
        "\n",
        "    # Look up the embeddings for the positive and negative context words.\n",
        "    # shape: (batch size, nbr contexts, emb dim)\n",
        "    context_embeddings = self.v_embedding(contexts)\n",
        "\n",
        "    # Transpose the tensor for matrix multiplication\n",
        "    # shape: (batch size, emb dim, nbr contexts)\n",
        "    context_embeddings = context_embeddings.view(n_batch, 1, self.embedding_dim)\n",
        "    context_embeddings = context_embeddings.transpose(1,2)\n",
        "\n",
        "    # Compute the dot products between target word embeddings and context\n",
        "    # embeddings. We express this as a batch matrix multiplication (bmm).\n",
        "    # shape: (batch size, 1, nbr contexts)\n",
        "    dots = tagets_embeddings.bmm(context_embeddings)\n",
        "\n",
        "    # View this result as a 2-dimensional tensor.\n",
        "    # shape: (batch size, nbr contexts)\n",
        "    dots = dots.view(n_batch, 1)\n",
        "\n",
        "    return dots\n",
        "\n"
      ],
      "id": "dvsmucKgxdXl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI0MnxChDQ7M"
      },
      "outputs": [],
      "source": [
        "print(pairs[:5])\n",
        "print(np.array([(word_to_ind[x[0]],word_to_ind[x[1]]) for x in pairs[:5]]))\n",
        "\n"
      ],
      "id": "UI0MnxChDQ7M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5gLPn_Ok8R9"
      },
      "outputs": [],
      "source": [
        "class SGNSTrainer:\n",
        "  def __init__(self, pairs, model, ns_table, n_epochs, k, word_to_ind, batch_size=2**10, optimizer='adam'):\n",
        "    super().__init__()\n",
        "    \"\"\"\n",
        "    ns_table - expanded table\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    # Make sure to understand what it means\n",
        "    # self.instance_gen = instance_gen\n",
        "    # self.max_words = params.get('max-words')\n",
        "    # self.n_ns = params['n-neg-samples']\n",
        "\n",
        "    self.model = model.to(device) # .cuda()#\n",
        "    self.n_epochs = n_epochs\n",
        "    self.batch_size = batch_size\n",
        "    pairs = np.array(pairs)\n",
        "    self.pairs = pairs\n",
        "    self.k = k\n",
        "    self.context_generator = ContextGenerator(ns_table=ns_table, k = self.k)\n",
        "    self.word_to_ind = word_to_ind\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "      self.optimizer = torch.optim.Adam(self.model.parameters())#, lr=params['lr'])\n",
        "    elif optimizer == 'sgd':\n",
        "      self.optimizer = torch.optim.SGD(self.model.parameters())#, lr=params['lr'])\n",
        "\n",
        "    # We'll use a binary cross-entropy loss, since we have a binary classification problem:\n",
        "    # distinguishing positive from negative contexts.\n",
        "    self.loss = nn.BCEWithLogitsLoss()\n",
        "    self.epoch = 0\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    import time\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    def convert_batch_of_words_to_inds(batch, word_to_ind):\n",
        "        return np.array([(word_to_ind[x[0]],word_to_ind[x[1]], x[2]) for x in batch])\n",
        "\n",
        "    batch_size = self.batch_size\n",
        "    t0 = time.time()\n",
        "\n",
        "    losses_list = []\n",
        "\n",
        "    for epoch in range(self.n_epochs):\n",
        "      print(f'Epoch {epoch+1}.')\n",
        "\n",
        "      pairs = self.pairs.copy()\n",
        "      np.random.shuffle(pairs)\n",
        "      j = 0\n",
        "\n",
        "      # constants for later verbosity\n",
        "      loss_in_epoch = 0\n",
        "      n_pairs_epoch = 0\n",
        "\n",
        "      while (j+1)*batch_size<len(pairs):\n",
        "\n",
        "        # get a batch of examples\n",
        "        pairs_in_batch = pairs[(j*batch_size):((j+1)*batch_size)]\n",
        "        batch_of_examples = context_generator.create_batch(true_pairs=pairs_in_batch)\n",
        "\n",
        "        # convert from words to inds\n",
        "        batch_of_examples = convert_batch_of_words_to_inds(batch_of_examples, self.word_to_ind)\n",
        "\n",
        "\n",
        "        targets = batch_of_examples[:,0].astype(int)\n",
        "        contexts = batch_of_examples[:,1].astype(int)\n",
        "        y = batch_of_examples[:,2].astype(int)\n",
        "\n",
        "        # convert to tensors\n",
        "        targets = torch.as_tensor(targets).to(device)\n",
        "        contexts = torch.as_tensor(contexts).to(device)\n",
        "        y = torch.as_tensor(y).to(device) # .unsqueeze(1)\n",
        "\n",
        "        # update j\n",
        "        j+=1\n",
        "\n",
        "        # clear grads from optimizer\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Compute the output from the model.\n",
        "        # That is, the dot products between target embeddings\n",
        "        # and context embeddings.\n",
        "        scores = self.model(targets, contexts).squeeze(1)\n",
        "        loss = self.loss(scores, y.float())\n",
        "\n",
        "        # Compute gradients and update the embeddings.\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # We'll print some diagnostics periodically.\n",
        "        loss_in_epoch += loss.item()\n",
        "        n_pairs_epoch += (batch_size+1)*self.k\n",
        "        losses_list.append(loss.item())\n",
        "        if ((j % 200) == 0):\n",
        "          print(f'iter {j}, loss {loss.item()}')\n",
        "\n",
        "      # Print diagnostics\n",
        "      t1 = time.time()\n",
        "      print(f'epoch: {epoch}, loss: {loss_in_epoch / n_pairs_epoch:.4f}, time: {t1-t0:.2f}')\n",
        "      t0 = time.time()\n",
        "\n",
        "      plt.plot(losses_list)\n",
        "      plt.title(f'Loss until the end of Epoch {epoch}')\n",
        "      plt.show()\n",
        "\n",
        "      self.epoch += 1\n"
      ],
      "id": "I5gLPn_Ok8R9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE09Aqyhk8Pm"
      },
      "outputs": [],
      "source": [
        "model = None\n",
        "\n",
        "def main():\n",
        "    global model\n",
        "\n",
        "    model = IP2Vec(vocab_size,embedding_dim=128, word_to_ind=word_to_ind)\n",
        "    trainer = SGNSTrainer(pairs = pairs, model = model, word_to_ind = word_to_ind, batch_size = 2**9, ns_table = ns_table, n_epochs = 2, k = 20)\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "main()\n"
      ],
      "id": "RE09Aqyhk8Pm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNeRFcBcD14P"
      },
      "outputs": [],
      "source": [
        "model.get_ip_embeddings('24.251.45.232')\n"
      ],
      "id": "tNeRFcBcD14P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkWx80liCZXG"
      },
      "outputs": [],
      "source": [
        "model.get_ip_embeddings('109.66.135.125')\n",
        "# model.get_ip_embeddings('36982')\n"
      ],
      "id": "nkWx80liCZXG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4N7hBBU-Wx0"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def get_embeddings(ip):\n",
        "  s = model.get_ip_embeddings(ip)\n",
        "  return preprocessing.normalize(s.cpu().detach().numpy(), norm='l2')\n",
        "\n",
        "entities = [x[0] for x in ns_table]\n",
        "\n",
        "vectors_of_embeddings = np.array([get_embeddings(x) for x in entities]).reshape(len(entities), 128)\n",
        "print(vectors_of_embeddings.shape)\n",
        "\n",
        "print('Example of normalized embeddings:')\n",
        "print(get_embeddings(entities[7]))\n",
        "\n",
        "for perplexity_ in [25,50,75,100,200]:\n",
        "  tsne_x = TSNE(n_components=2, perplexity=perplexity_).fit_transform(vectors_of_embeddings)\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.scatter(tsne_x[:,0], tsne_x[:,1])\n",
        "  plt.title(f't-SNE plot with perplexity of {perplexity_}')\n",
        "  plt.show()"
      ],
      "id": "H4N7hBBU-Wx0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIYI--NzixA2"
      },
      "outputs": [],
      "source": [],
      "id": "oIYI--NzixA2"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}